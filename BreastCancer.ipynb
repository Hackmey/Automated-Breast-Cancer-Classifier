{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":999617,"sourceType":"datasetVersion","datasetId":209316},{"sourceId":10660709,"sourceType":"datasetVersion","datasetId":6601937},{"sourceId":10708689,"sourceType":"datasetVersion","datasetId":6636847},{"sourceId":10713660,"sourceType":"datasetVersion","datasetId":6640608},{"sourceId":10713959,"sourceType":"datasetVersion","datasetId":6640829},{"sourceId":10763856,"sourceType":"datasetVersion","datasetId":6676833},{"sourceId":10811531,"sourceType":"datasetVersion","datasetId":6711579},{"sourceId":10821320,"sourceType":"datasetVersion","datasetId":6718773},{"sourceId":10859586,"sourceType":"datasetVersion","datasetId":6745850}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# Define the dataset path (adjust according to your Kaggle dataset path)\ndataset_path = \"/kaggle/input/breakhis\"\n\n# Display directory structure\nfor root, dirs, files in os.walk(dataset_path):\n    print(root)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install opencv-python-headless\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Preprocessing functions\ndef resize_image(image, target_size=(250, 250)):\n    \"\"\"\n    Resize the image to the target size (250x250) while maintaining all content.\n    \"\"\"\n    resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n    return resized_image\n\n\ndef apply_gaussian_filter(image, kernel_size=5, sigma=1.0):\n    \"\"\"Apply Gaussian filter for denoising.\"\"\"\n    denoised_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)\n    return denoised_image\n\ndef apply_clahe(image):\n    \"\"\"Apply CLAHE to enhance contrast while retaining original color.\"\"\"\n    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)  # Convert to LAB color space\n    l_channel, a_channel, b_channel = cv2.split(lab_image)  # Split into L, A, and B channels\n    \n    # Apply CLAHE only to the L channel (lightness)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    l_channel = clahe.apply(l_channel)\n    \n    # Merge the CLAHE enhanced L channel with the original A and B channels\n    clahe_image = cv2.merge((l_channel, a_channel, b_channel))\n    \n    # Convert back to BGR color space\n    enhanced_image = cv2.cvtColor(clahe_image, cv2.COLOR_LAB2BGR)\n    final_image = cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB)\n    return final_image\n\ndef preprocess_image(image_path, target_size=(250, 250), kernel_size=5, sigma=1.0):\n    \"\"\"Complete preprocessing pipeline for a single image.\"\"\"\n    image = cv2.imread(image_path)\n    \n    # Step 1: Resize image\n    resized_image = resize_image(image, target_size)\n    \n    # Step 2: Apply Gaussian filter (denoising)\n    denoised_image = apply_gaussian_filter(resized_image, kernel_size, sigma)\n    \n    # Step 3: Apply CLAHE (contrast enhancement while retaining colors)\n    preprocessed_image = apply_clahe(denoised_image)\n    \n    return preprocessed_image\n\ndef preprocess_dataset(input_dir, output_dir, target_size=(250, 250)):\n    \"\"\"Preprocess all images in the dataset.\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    for root, dirs, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n                # Input and output paths\n                image_path = os.path.join(root, file)\n                relative_path = os.path.relpath(root, input_dir)\n                output_path = os.path.join(output_dir, relative_path)\n                \n                # Ensure output directory exists\n                os.makedirs(output_path, exist_ok=True)\n                \n                # Preprocess and save the image\n                preprocessed_image = preprocess_image(image_path, target_size)\n                output_file_path = os.path.join(output_path, file)\n                cv2.imwrite(output_file_path, preprocessed_image)\n                print(f\"Processed: {output_file_path}\")\n\n\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\n# Preprocessing functions\ndef resize_image(image, target_size=(250, 250)):\n    \"\"\"\n    Resize the image to the target size (250x250) while maintaining all content.\n    \"\"\"\n    resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n    return resized_image\n\n\ndef apply_gaussian_filter(image, kernel_size=5, sigma=1.0):\n    \"\"\"Apply Gaussian filter for denoising.\"\"\"\n    denoised_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), sigma)\n    return denoised_image\n\ndef apply_clahe(image):\n    \"\"\"Apply CLAHE to enhance contrast while retaining original color.\"\"\"\n    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)  # Convert to LAB color space\n    l_channel, a_channel, b_channel = cv2.split(lab_image)  # Split into L, A, and B channels\n    \n    # Apply CLAHE only to the L channel (lightness)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    l_channel = clahe.apply(l_channel)\n    \n    # Merge the CLAHE enhanced L channel with the original A and B channels\n    clahe_image = cv2.merge((l_channel, a_channel, b_channel))\n    \n    # Convert back to BGR color space\n    enhanced_image = cv2.cvtColor(clahe_image, cv2.COLOR_LAB2BGR)\n    final_image = cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2RGB)\n    return final_image\n\ndef preprocess_image(image_path, target_size=(250, 250), kernel_size=5, sigma=1.0):\n    \"\"\"Complete preprocessing pipeline for a single image.\"\"\"\n    image = cv2.imread(image_path)\n    \n    # Step 1: Resize image\n    resized_image = resize_image(image, target_size)\n    \n    # Step 2: Apply Gaussian filter (denoising)\n    denoised_image = apply_gaussian_filter(resized_image, kernel_size, sigma)\n    \n    # Step 3: Apply CLAHE (contrast enhancement while retaining colors)\n    preprocessed_image = apply_clahe(denoised_image)\n    \n    return preprocessed_image\n\ndef preprocess_dataset(input_dir, output_dir, target_size=(250, 250)):\n    \"\"\"Preprocess all images in the dataset.\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    count = 0\n    for root, dirs, files in os.walk(input_dir):\n        \n        for file in files:\n            if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n                # Input and output paths\n                image_path = os.path.join(root, file)\n                relative_path = os.path.relpath(root, input_dir)\n                output_path = os.path.join(output_dir, relative_path)\n                \n                # Ensure output directory exists\n                os.makedirs(output_path, exist_ok=True)\n                \n                # Preprocess and save the image\n                preprocessed_image = preprocess_image(image_path, target_size)\n                output_file_path = os.path.join(output_path, file)\n                cv2.imwrite(output_file_path, preprocessed_image)\n                print(f\"{count}\", end = \" \")\n                count += 1\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage\ninput_dir = dataset_path\noutput_dir = \"/kaggle/working/\"\npreprocess_dataset(input_dir, output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#**Visualisation of preprecessed images**\n\n\nimport matplotlib.pyplot as plt\n\n# Function for visualization\ndef visualize_preprocessing(input_dir, target_size=(250, 250), kernel_size=5, sigma=1.0, num_samples=10):\n    \"\"\"Visualize the preprocessing pipeline with original and processed images.\"\"\"\n    # Collect a list of image paths\n    image_paths = []\n    for root, _, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith((\".png\", \".jpg\", \".jpeg\")):\n                image_paths.append(os.path.join(root, file))\n    \n    # Randomly select samples for visualization\n    selected_paths = np.random.choice(image_paths, size=num_samples, replace=False)\n    \n    # Prepare visualization\n    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 5 * num_samples))\n    for i, image_path in enumerate(selected_paths):\n        # Read and preprocess image\n        original_image = cv2.imread(image_path)\n        preprocessed_image = preprocess_image(image_path, target_size, kernel_size, sigma)\n        \n        # Convert BGR to RGB for visualization\n        original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n        preprocessed_image_rgb = cv2.cvtColor(preprocessed_image, cv2.COLOR_BGR2RGB)\n        \n        # Display original and preprocessed images\n        axes[i, 0].imshow(original_image_rgb)\n        axes[i, 0].set_title(\"Original Image\")\n        axes[i, 0].axis(\"off\")\n        \n        axes[i, 1].imshow(preprocessed_image_rgb)\n        axes[i, 1].set_title(\"Preprocessed Image\")\n        axes[i, 1].axis(\"off\")\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example usage\ninput_dir = \"/kaggle/input/breakhis\"\nvisualize_preprocessing(input_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\n\ndef preprocess_image_with_visualization(image_path, target_size=(250, 250), kernel_size=5, sigma=1.0):\n    \"\"\"Complete preprocessing pipeline for a single image with output visualization.\"\"\"\n    image = cv2.imread(image_path)\n    plt.figure(figsize=(12, 8))\n    plt.subplot(1, 4, 1)\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.title(\"Original Image\")\n    plt.axis('off')\n\n    # Step 1: Resize image\n    resized_image = resize_image(image, target_size)\n    plt.figure(figsize=(12, 8))\n    plt.subplot(1, 4, 2)\n    plt.imshow(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB))\n    plt.title(\"Step 1: Resized Image\")\n    plt.axis('off')\n\n    # Step 2: Apply Gaussian filter (denoising)\n    denoised_image = apply_gaussian_filter(resized_image, kernel_size, 1.0)\n    plt.subplot(1, 4, 3)\n    plt.imshow(cv2.cvtColor(denoised_image, cv2.COLOR_BGR2RGB))\n    plt.title(\"Step 2: Denoised Image\")\n    plt.axis('off')\n\n    # Step 3: Apply CLAHE (contrast enhancement while retaining colors)\n    preprocessed_image = apply_clahe(denoised_image)\n    plt.subplot(1, 4, 4)\n    plt.imshow(cv2.cvtColor(preprocessed_image, cv2.COLOR_BGR2RGB))\n    plt.title(\"Step 3: CLAHE Enhanced Image\")\n    plt.axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n    return preprocessed_image\n\n# Example usage\nimage_path = \"/kaggle/input/breakhis/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/benign/SOB/adenosis/SOB_B_A_14-22549AB/100X/SOB_B_A-14-22549AB-100-001.png\"\npreprocessed_image = preprocess_image_with_visualization(image_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\n# Import pre-trained models and their preprocessing functions\nfrom tensorflow.keras.applications import ResNet50, VGG16, Xception\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\nfrom tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n\n# --- 1. Load Pre-trained Models (without top layers) ---\nresnet_model = ResNet50(weights=\"/kaggle/input/transefermodels/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, pooling=\"avg\")\nvgg_model    = VGG16(weights=\"/kaggle/input/transefermodels/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, pooling=\"avg\")\nxception_model = Xception(weights=\"/kaggle/input/transefermodels/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, pooling=\"avg\")\n\n# --- 2. Define Helper Functions ---\n\ndef split_image(image):\n    \"\"\"\n    Given a 250x250 image, split it into 4 equal quadrants\n    and then resize each quadrant to 224x224.\n    \"\"\"\n    h, w, _ = image.shape\n    mid_h, mid_w = h // 2, w // 2\n    quadrants = [\n        image[0:mid_h, 0:mid_w],\n        image[0:mid_h, mid_w:w],\n        image[mid_h:h, 0:mid_w],\n        image[mid_h:h, mid_w:w]\n    ]\n    return [cv2.resize(q, (224, 224)) for q in quadrants]\n\ndef extract_features_from_sub_images(sub_images):\n    \"\"\"\n    Process each sub-image through the three models and\n    concatenate the resulting feature vectors.\n    Returns a NumPy array of shape (4, combined_feature_length).\n    \"\"\"\n    features_list = []\n    for sub_img in sub_images:\n        sub_img_batch = np.expand_dims(sub_img, axis=0)\n        feat_resnet   = resnet_model.predict(resnet_preprocess(sub_img_batch), verbose=0)\n        feat_vgg      = vgg_model.predict(vgg_preprocess(sub_img_batch), verbose=0)\n        feat_xception = xception_model.predict(xception_preprocess(sub_img_batch), verbose=0)\n        combined = np.concatenate([feat_resnet, feat_vgg, feat_xception], axis=1)\n        features_list.append(combined)\n    return np.array(features_list)\n\ndef process_subclass(subclass_dir, main_class, tumor_type, output_base):\n    \"\"\"\n    Process all images in a subclass folder.\n    For each image:\n      - Load (and convert to RGB), ensure size 250x250,\n      - Split into 4 sub-images,\n      - Extract features,\n      - Append the features and label.\n    Finally, save the features and labels as NumPy files.\n    \"\"\"\n    features_list = []\n    labels_list = []\n    for root, dirs, files in os.walk(subclass_dir):\n        for file in files:\n            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                image_path = os.path.join(root, file)\n                image = cv2.imread(image_path)\n                if image is None:\n                    print(f\"Skipping {image_path}: could not load.\")\n                    continue\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                if image.shape[0] != 250 or image.shape[1] != 250:\n                    image = cv2.resize(image, (250, 250))\n                sub_imgs = split_image(image)\n                feats = extract_features_from_sub_images(sub_imgs)\n                features_list.append(feats)\n                labels_list.append(f\"{main_class}_{tumor_type}\")\n                print(f\"Processed: {image_path}\")\n    if features_list:\n        features_array = np.array(features_list, dtype=np.float32)\n        labels_array = np.array(labels_list)\n        os.makedirs(output_base, exist_ok=True)\n        np.save(os.path.join(output_base, f\"features_{main_class}_{tumor_type}.npy\"), features_array)\n        np.save(os.path.join(output_base, f\"labels_{main_class}_{tumor_type}.npy\"), labels_array)\n        print(f\"Saved {len(features_list)} images for subclass '{main_class}_{tumor_type}'\")\n    else:\n        print(f\"No images found in {main_class}_{tumor_type}\")\n\n# --- 3. Process All Benign Images ---\n\nbase_dir = \"/kaggle/working/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/\"\nbenign_dir = os.path.join(base_dir, \"benign\")\noutput_base = \"/kaggle/working/benign_features/\"\n\nif os.path.isdir(benign_dir):\n    # Assume structure: benign/SOB/<tumor_type>/\n    for sob_folder in os.listdir(benign_dir):\n        sob_path = os.path.join(benign_dir, sob_folder)\n        if not os.path.isdir(sob_path): continue\n        for tumor_type in os.listdir(sob_path):\n            tumor_dir = os.path.join(sob_path, tumor_type)\n            print(tumor_dir, \"  \", type(tumor_dir))\n            if not os.path.isdir(tumor_dir): continue\n            if tumor_dir != \"/kaggle/working/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/benign/SOB/fibroadenoma\" : continue\n            print(f\"\\n--- Processing benign subclass: {tumor_type} ---\")\n            process_subclass(tumor_dir, \"benign\", tumor_type, output_base)\nelse:\n    print(\"Benign directory not found.\")\n\nprint(\"✅ Benign extraction complete!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nfeatures = np.load(\"breakhis_features.npy\")\nlabels = np.load(\"breakhis_labels.npy\")\n\nprint(\"Feature Array Shape:\", features.shape)\nprint(\"Labels Shape:\", labels.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T03:43:54.141845Z","iopub.execute_input":"2025-02-24T03:43:54.142121Z","iopub.status.idle":"2025-02-24T03:43:54.235379Z","shell.execute_reply.started":"2025-02-24T03:43:54.142092Z","shell.execute_reply":"2025-02-24T03:43:54.233898Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0936be7afa72>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"breakhis_features.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"breakhis_labels.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'breakhis_features.npy'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'breakhis_features.npy'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"print(\"First Image Features:\\n\", features[0])\nprint(\"First Image Label:\", labels[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm  # progress bar\n\n# Import pre-trained models and their preprocessing functions\nfrom tensorflow.keras.applications import ResNet50, VGG16, Xception\nfrom tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\nfrom tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\nfrom tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n\n# --- 1. Load Pre-trained Models ---\nresnet_model = ResNet50(weights=\"/kaggle/input/transefermodels/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, pooling=\"avg\")\nvgg_model    = VGG16(weights=\"/kaggle/input/transefermodels/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, pooling=\"avg\")\nxception_model = Xception(weights=\"/kaggle/input/transefermodels/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\", include_top=False, pooling=\"avg\")\n\n# --- 2. Helper Functions (same as in Code 1) ---\ndef split_image(image):\n    h, w, _ = image.shape\n    mid_h, mid_w = h // 2, w // 2\n    quadrants = [\n        image[0:mid_h, 0:mid_w],\n        image[0:mid_h, mid_w:w],\n        image[mid_h:h, 0:mid_w],\n        image[mid_h:h, mid_w:w]\n    ]\n    return [cv2.resize(q, (224, 224)) for q in quadrants]\n\ndef extract_features_from_sub_images(sub_images):\n    features_list = []\n    for sub_img in sub_images:\n        sub_img_batch = np.expand_dims(sub_img, axis=0)\n        feat_resnet = resnet_model.predict(resnet_preprocess(sub_img_batch), verbose=0)\n        feat_vgg = vgg_model.predict(vgg_preprocess(sub_img_batch), verbose=0)\n        feat_xception = xception_model.predict(xception_preprocess(sub_img_batch), verbose=0)\n        combined = np.concatenate([feat_resnet, feat_vgg, feat_xception], axis=1)\n        features_list.append(combined)\n    return np.array(features_list)\n\ndef process_subclass(subclass_dir, main_class, tumor_type, output_base):\n    image_paths = []\n    for root, dirs, files in os.walk(subclass_dir):\n        for file in files:\n            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n                image_paths.append(os.path.join(root, file))\n    features_list = []\n    labels_list = []\n    for image_path in tqdm(image_paths, desc=f\"Processing {main_class}_{tumor_type}\"):\n        image = cv2.imread(image_path)\n        if image is None:\n            print(f\"Skipping {image_path}: could not load.\")\n            continue\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if image.shape[0] != 250 or image.shape[1] != 250:\n            image = cv2.resize(image, (250, 250))\n        sub_imgs = split_image(image)\n        feats = extract_features_from_sub_images(sub_imgs)\n        features_list.append(feats)\n        labels_list.append(f\"{main_class}_{tumor_type}\")\n    if features_list:\n        features_array = np.array(features_list, dtype=np.float32)\n        labels_array = np.array(labels_list)\n        os.makedirs(output_base, exist_ok=True)\n        np.save(os.path.join(output_base, f\"features_{main_class}_{tumor_type}.npy\"), features_array)\n        np.save(os.path.join(output_base, f\"labels_{main_class}_{tumor_type}.npy\"), labels_array)\n        print(f\"Saved {len(features_list)} images for subclass '{main_class}_{tumor_type}'\")\n    else:\n        print(f\"No images found in {main_class}_{tumor_type}\")\n\n# --- 3. Process Malignant Ductal and Lobular Subtypes ---\nbase_dir = \"/kaggle/working/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/\"\nmalignant_dir = os.path.join(base_dir, \"malignant\")\noutput_base = \"/kaggle/working/malignant_features/\"\n\nif os.path.isdir(malignant_dir):\n    # Assume structure: malignant/SOB/<tumor_type>/\n    for sob_folder in os.listdir(malignant_dir):\n        sob_path = os.path.join(malignant_dir, sob_folder)\n        if not os.path.isdir(sob_path): continue\n        for tumor_type in os.listdir(sob_path):\n            # Process only ductal and lobular carcinoma folders\n            if tumor_type.lower() in ['ductal_carcinoma']:\n                tumor_dir = os.path.join(sob_path, tumor_type)\n                if not os.path.isdir(tumor_dir): continue\n                print(f\"\\n--- Processing malignant subtype: {tumor_type} ---\")\n                process_subclass(tumor_dir, \"malignant\", tumor_type, output_base)\nelse:\n    print(\"Malignant directory not found.\")\n\nprint(\"✅ Malignant ductal and lobular extraction complete!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r malignant_features.zip /kaggle/working/malignant_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/output.zip /kaggle/working/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nimport pickle\n\ndef merge_features_labels(features_dir):\n    \"\"\"\n    Given a directory where feature and label .npy files are stored,\n    this function searches for all files starting with 'features_' and then\n    loads each corresponding labels file (which is expected to have the same name\n    with 'features_' replaced by 'labels_').\n    \n    The arrays from all files are concatenated along the first axis.\n    \n    Returns:\n        all_features: NumPy array of shape (N, 4, D)\n        all_labels: NumPy array of shape (N,)\n    \"\"\"\n    features_files = glob.glob(os.path.join(features_dir, \"features_*.npy\"))\n    all_features_list = []\n    all_labels_list = []\n    \n    for f_file in features_files:\n        # Construct the corresponding label file name.\n        l_file = f_file.replace(\"features_\", \"labels_\")\n        print(l_file, \" before\")\n        if l_file == \"/kaggle/input/featuresnlabels/features/features_malignant_mucinous_carcinoma (1).npy\":\n            l_file = \"/kaggle/input/featuresnlabels/features/features_malignant_mucinous_carcinoma.npy\"\n            print(l_file, \" after\")\n        if os.path.exists(l_file):\n            feats = np.load(f_file)\n            labs = np.load(l_file)\n            all_features_list.append(feats)\n            all_labels_list.append(labs)\n        else:\n            print(f\"Warning: Corresponding label file for {f_file} not found.\")\n    \n    if all_features_list:\n        all_features = np.concatenate(all_features_list, axis=0)\n        all_labels = np.concatenate(all_labels_list, axis=0)\n        return all_features, all_labels\n    else:\n        return None, None\n\ndef main():\n    # ------------------------------\n    # 1. Merge Features and Labels\n    # ------------------------------\n    features_dir = \"/kaggle/input/featss/features/\"\n    all_features, all_labels = merge_features_labels(features_dir)\n    \n    if all_features is None or all_labels is None:\n        print(\"No features or labels found! Please check your file paths.\")\n        return\n\n    # all_features is expected to have shape (N, 4, D)\n    print(\"Merged features shape:\", all_features.shape)\n    print(\"Merged labels shape:\", all_labels.shape)\n    \n    # ------------------------------\n    # 2. Feature Fusion\n    # ------------------------------\n    # Fuse the features from the 4 sub-images by taking the mean along axis 1.\n    # This yields an image-level feature vector for each image, with shape (N, D).\n    fused_features = np.mean(all_features, axis=1)\n    print(\"After fusion, fused features shape:\", fused_features.shape)\n    \n    # Check if fused_features is 2D. ExtraTreesClassifier expects a 2D array.\n    if fused_features.ndim != 2:\n        print(\"Fused features are not 2D (ndim =\", fused_features.ndim, \"). Reshaping now.\")\n        fused_features = fused_features.reshape(fused_features.shape[0], -1)\n    print(\"Final fused features shape (for classifier):\", fused_features.shape)\n    \n    # ------------------------------\n    # 3. Feature Selection Using ExtraTreesClassifier\n    # ------------------------------\n    etc = ExtraTreesClassifier(n_estimators=100, random_state=42)\n    etc.fit(fused_features, all_labels)\n    \n    # Use SelectFromModel to select features with importance above the median.\n    sfm = SelectFromModel(etc, threshold=\"median\", prefit=True)\n    selected_features = sfm.transform(fused_features)\n    \n    print(\"Original fused feature dimension:\", fused_features.shape[1])\n    print(\"Dimension after feature selection:\", selected_features.shape[1])\n    \n    # ------------------------------\n    # 4. Save the Selected Features and the Feature Selection Model\n    # ------------------------------\n    selected_features_path = \"/kaggle/working/selected_features.npy\"\n    model_path = \"/kaggle/working/feature_selection_model.pkl\"\n    \n    np.save(selected_features_path, selected_features)\n    print(\"Saved selected features to:\", selected_features_path)\n    \n    with open(model_path, \"wb\") as f:\n        pickle.dump(sfm, f)\n    print(\"Saved feature selection model to:\", model_path)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pickle\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n\n# Scikit-learn imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n                             confusion_matrix, log_loss, classification_report, roc_auc_score)\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\n\n# Additional models\nfrom xgboost import XGBClassifier\n\ndef merge_features_labels(features_dir):\n    \"\"\"\n    Searches the given directory for all files starting with 'features_'.\n    For each such file, it loads the corresponding 'labels_' file.\n    It then concatenates all feature arrays and label arrays along axis 0.\n    \n    Returns:\n        all_features: NumPy array of shape (N, 4, D) or (N, 4, 1, D)\n        all_labels: NumPy array of shape (N,)\n    \"\"\"\n    print(\"Starting to merge feature and label files...\")\n    features_files = glob.glob(os.path.join(features_dir, \"features_*.npy\"))\n    features_list = []\n    labels_list = []\n    \n    for f_file in tqdm(features_files, desc=\"Merging feature files\"):\n        # Construct the corresponding labels file name.\n        l_file = f_file.replace(\"features_\", \"labels_\")\n        if os.path.exists(l_file):\n            feat = np.load(f_file)\n            lab = np.load(l_file)\n            features_list.append(feat)\n            labels_list.append(lab)\n        else:\n            print(f\"Warning: Label file for {f_file} not found.\")\n    \n    if features_list:\n        all_features = np.concatenate(features_list, axis=0)\n        all_labels = np.concatenate(labels_list, axis=0)\n        return all_features, all_labels\n    else:\n        return None, None\n\ndef main():\n    # -------------------------------------------------------\n    # 1. Merge Features and Labels from the Folder\n    # -------------------------------------------------------\n    features_dir = \"/kaggle/input/featss/features/\"\n    all_features, all_labels = merge_features_labels(features_dir)\n    \n    if all_features is None or all_labels is None:\n        print(\"No features or labels found! Please check your file paths.\")\n        return\n\n    print(\"Merged features shape:\", all_features.shape)  # e.g., (N, 4, 1, D)\n    print(\"Merged labels shape:\", all_labels.shape)      # e.g., (N,)\n    \n    # -------------------------------------------------------\n    # 2. Feature Fusion: Average the sub-image features for each image\n    # -------------------------------------------------------\n    print(\"Starting feature fusion...\")\n    if all_features.ndim == 4 and all_features.shape[2] == 1:\n        all_features = np.squeeze(all_features, axis=2)  # Now shape: (N, 4, D)\n    fused_features = np.mean(all_features, axis=1)  # Shape: (N, D)\n    if fused_features.ndim != 2:\n        fused_features = fused_features.reshape(fused_features.shape[0], -1)\n    print(\"Fused features shape (for classifier):\", fused_features.shape)\n    \n    # -------------------------------------------------------\n    # 3. Encode Class Labels into Numeric Values\n    # -------------------------------------------------------\n    print(\"Encoding class labels...\")\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(all_labels)\n    print(\"Unique classes:\", le.classes_)  # Expected 8 unique class names\n    \n    # -------------------------------------------------------\n    # 4. Split Data into Training and Testing Sets (Stratified)\n    # -------------------------------------------------------\n    print(\"Splitting data into training and testing sets...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        fused_features, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n    )\n    print(\"Training set shape:\", X_train.shape)\n    print(\"Test set shape:\", X_test.shape)\n    \n    # -------------------------------------------------------\n    # 5. Train Individual Base Models\n    # -------------------------------------------------------\n    print(\"Training individual base models...\")\n    clf_lr = LogisticRegression(max_iter=1000, random_state=42)\n    clf_svc = SVC(probability=True, random_state=42)\n    clf_extra = ExtraTreesClassifier(n_estimators=200, random_state=42)\n    clf_ridge = RidgeClassifier(random_state=42)\n    clf_xgb = XGBClassifier(n_estimators=200, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n\n    \n    base_models = [(\"Logistic Regression\", clf_lr),\n                   (\"SVC\", clf_svc),\n                   (\"Extra Trees\", clf_extra),\n                   (\"Ridge Classifier\", clf_ridge),\n                   (\"XGBoost\", clf_xgb),]\n    \n    for name, clf in tqdm(base_models, desc=\"Training base models\"):\n        clf.fit(X_train, y_train)\n    \n    # -------------------------------------------------------\n    # 6. Build a Voting Ensemble Model (Soft Voting)\n    # -------------------------------------------------------\n    print(\"Building voting ensemble...\")\n    voting_clf = VotingClassifier(estimators=[\n        ('lr', clf_lr),\n        ('svc', clf_svc),\n        ('extra', clf_extra),\n        ('ridge', clf_ridge),\n        ('xgb', clf_xgb)\n    ], voting='soft')\n    \n    print(\"Evaluating ensemble with cross-validation...\")\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=skf, scoring='accuracy')\n    print(\"\\nVoting Ensemble CV Accuracy: {:.4f}\".format(cv_scores.mean()))\n    \n    print(\"Training voting ensemble on full training set...\")\n    voting_clf.fit(X_train, y_train)\n    \n    # -------------------------------------------------------\n    # 7. Evaluate Model Performance on the Test Set\n    # -------------------------------------------------------\n    print(\"Evaluating ensemble on test set...\")\n    y_pred = voting_clf.predict(X_test)\n    y_prob = voting_clf.predict_proba(X_test)\n    \n    # Compute evaluation metrics (using macro averaging for multi-class)\n    metrics = {}\n    metrics['accuracy'] = accuracy_score(y_test, y_pred)\n    metrics['precision'] = precision_score(y_test, y_pred, average='macro')\n    metrics['recall'] = recall_score(y_test, y_pred, average='macro')\n    metrics['f1_score'] = f1_score(y_test, y_pred, average='macro')\n    metrics['mcr'] = 1 - metrics['accuracy']\n    metrics['log_loss'] = log_loss(y_test, y_prob)\n    try:\n        metrics['auc_roc'] = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')\n    except Exception as e:\n        metrics['auc_roc'] = None\n\n    cm = confusion_matrix(y_test, y_pred)\n    metrics['confusion_matrix'] = cm\n    \n    print(\"\\nEvaluation Metrics:\")\n    print(\"  Accuracy:      {:.4f}\".format(metrics['accuracy']))\n    print(\"  Precision:     {:.4f}\".format(metrics['precision']))\n    print(\"  Recall:        {:.4f}\".format(metrics['recall']))\n    print(\"  F1-score:      {:.4f}\".format(metrics['f1_score']))\n    print(\"  Misclassification Rate: {:.4f}\".format(metrics['mcr']))\n    print(\"  Log Loss:      {:.4f}\".format(metrics['log_loss']))\n    print(\"  AUC-ROC:       {:.4f}\".format(metrics['auc_roc'] if metrics['auc_roc'] is not None else -1))\n    \n    print(\"\\nConfusion Matrix:\")\n    print(cm)\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=le.classes_))\n    \n    # -------------------------------------------------------\n    # 8. Save the Final Ensemble Model\n    # -------------------------------------------------------\n    model_path = \"/kaggle/working/voting_ensemble.pkl\"\n    with open(model_path, \"wb\") as f:\n        pickle.dump(voting_clf, f)\n    print(\"Voting ensemble model saved to:\", model_path)\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\ndef check_feature_fusion(features_array):\n    \"\"\"\n    Checks if the feature array is fused or not.\n    \n    - If fused, the expected shape is (N, D).\n    - If not fused, the shape might be (N, 4, D) or (N, 4, 1, D).\n    \"\"\"\n    shape = features_array.shape\n    print(\"Feature array shape:\", shape)\n    \n    if features_array.ndim == 2:\n        print(\"The features are fused (2D array: (N, D)).\")\n    elif features_array.ndim == 3:\n        if shape[1] == 4:\n            print(\"The features are NOT fused (3D array: (N, 4, D)).\")\n        else:\n            print(\"The features are 3D but do not match expected unfused shape.\")\n    elif features_array.ndim == 4:\n        if shape[1] == 4 and shape[2] == 1:\n            print(\"The features are NOT fused (4D array: (N, 4, 1, D)).\")\n        else:\n            print(\"The features are 4D but do not match expected unfused shape.\")\n    else:\n        print(\"Unexpected feature array shape.\")\n\n# Example usage:\n# Replace 'path/to/features.npy' with your actual feature file path.\nfeatures_path = \"/kaggle/input/featss/features/features_benign_fibroadenoma.npy\"\nfeatures = np.load(features_path)\n\ncheck_feature_fusion(features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pickle\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n\n# Scikit-learn imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Additional model\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\ndef merge_fused_features_labels(input_dir):\n    \"\"\"\n    Scans the given directory for files starting with \n    'fused_selected_features_' and loads their corresponding\n    'fused_selected_labels_' files.\n    \n    Returns:\n        all_features: NumPy array of shape (N, D)\n        all_labels: NumPy array of shape (N,)\n    \"\"\"\n    feat_files = glob.glob(os.path.join(input_dir, \"fused_selected_features_*.npy\"))\n    features_list = []\n    labels_list = []\n    \n    for feat_file in tqdm(feat_files, desc=\"Merging fused feature files\"):\n        label_file = feat_file.replace(\"fused_selected_features_\", \"fused_selected_labels_\")\n        if os.path.exists(label_file):\n            feats = np.load(feat_file)\n            labs = np.load(label_file)\n            features_list.append(feats)\n            labels_list.append(labs)\n        else:\n            print(f\"Warning: Label file for {feat_file} not found.\")\n    \n    if features_list:\n        all_features = np.concatenate(features_list, axis=0)\n        all_labels = np.concatenate(labels_list, axis=0)\n        return all_features, all_labels\n    else:\n        return None, None\n\ndef main():\n    # Use your input directory containing fused selected features and labels\n    input_dir = \"/kaggle/input/selected-feats/fused_selected_features/\"\n    all_features, all_labels = merge_fused_features_labels(input_dir)\n    \n    if all_features is None or all_labels is None:\n        print(\"No features or labels found. Please check the directory.\")\n        return\n    \n    print(\"Merged fused features shape:\", all_features.shape)  # e.g., (7909, D)\n    print(\"Merged labels shape:\", all_labels.shape)            # e.g., (7909,)\n    \n    # Encode labels (if not already numeric)\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(all_labels)\n    \n    # Split data into training and testing sets (80/20, stratified)\n    X_train, X_test, y_train, y_test = train_test_split(\n        all_features, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n    )\n    \n    # Define your base models\n    clf_lr = LogisticRegression(max_iter=1000, random_state=42)\n    clf_svc = SVC(probability=True, random_state=42)\n    clf_extra = ExtraTreesClassifier(n_estimators=200, random_state=42)\n    ridge = RidgeClassifier(random_state=42)\n    clf_ridge = CalibratedClassifierCV(ridge, cv=5)\n    clf_xgb = XGBClassifier(n_estimators=200, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n    clf_lgbm = LGBMClassifier(n_estimators=200, random_state=42)\n    \n    base_models = [\n        (\"SVC\", clf_svc)\n    ]\n    \n    # Create directory to save base models\n    base_model_dir = \"/kaggle/working/base_models/\"\n    os.makedirs(base_model_dir, exist_ok=True)\n    \n    print(\"Training and saving base models...\")\n    for name, clf in tqdm(base_models, desc=\"Training base models\"):\n        clf.fit(X_train, y_train)\n        model_file = os.path.join(base_model_dir, f\"{name}.pkl\")\n        with open(model_file, \"wb\") as f:\n            pickle.dump(clf, f)\n        print(f\"Saved {name} model to {model_file}\")\n    \n    print(\"All base models have been trained and saved.\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T07:48:01.608871Z","iopub.execute_input":"2025-02-21T07:48:01.609099Z","execution_failed":"2025-02-21T07:50:42.848Z"}},"outputs":[{"name":"stderr","text":"Merging fused feature files: 100%|██████████| 8/8 [00:00<00:00, 97.57it/s]","output_type":"stream"},{"name":"stdout","text":"Merged fused features shape: (7909, 4608)\nMerged labels shape: (7909,)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Training and saving base models...\n","output_type":"stream"},{"name":"stderr","text":"Training base models:   0%|          | 0/1 [00:00<?, ?it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pickle\nimport warnings\nfrom tqdm import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n\n# Scikit-learn imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Additional models\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\ndef merge_fused_features_labels(input_dir):\n    \"\"\"\n    Scans the given directory for files starting with \n    'fused_selected_features_' and loads their corresponding\n    'fused_selected_labels_' files.\n    \n    Returns:\n        all_features: NumPy array of shape (N, D)\n        all_labels: NumPy array of shape (N,)\n    \"\"\"\n    feat_files = glob.glob(os.path.join(input_dir, \"fused_selected_features_*.npy\"))\n    features_list = []\n    labels_list = []\n    \n    for feat_file in tqdm(feat_files, desc=\"Merging fused feature files\"):\n        label_file = feat_file.replace(\"fused_selected_features_\", \"fused_selected_labels_\")\n        if os.path.exists(label_file):\n            feats = np.load(feat_file)\n            labs = np.load(label_file)\n            features_list.append(feats)\n            labels_list.append(labs)\n        else:\n            print(f\"Warning: Label file for {feat_file} not found.\")\n    \n    if features_list:\n        all_features = np.concatenate(features_list, axis=0)\n        all_labels = np.concatenate(labels_list, axis=0)\n        return all_features, all_labels\n    else:\n        return None, None\n\ndef main():\n    # -------------------------------------------------------\n    # 1. Merge All Fused Selected Feature Files and Labels\n    # -------------------------------------------------------\n    print(\"Step 1: Merging fused selected features and labels...\")\n    input_dir = \"/kaggle/input/selected-feats/fused_selected_features/\"\n    all_features, all_labels = merge_fused_features_labels(input_dir)\n    \n    if all_features is None or all_labels is None:\n        print(\"No features or labels found. Please check the directory.\")\n        return\n    \n    print(\"Merged fused features shape:\", all_features.shape)\n    print(\"Merged labels shape:\", all_labels.shape)\n    \n    # -------------------------------------------------------\n    # 2. Encode Class Labels into Numeric Values\n    # -------------------------------------------------------\n    print(\"Step 2: Encoding class labels...\")\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(all_labels)\n    print(\"Unique classes:\", le.classes_)\n    \n    # -------------------------------------------------------\n    # 3. Split Data into Training and Testing Sets (Stratified)\n    # -------------------------------------------------------\n    print(\"Step 3: Splitting data into training and test sets...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        all_features, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n    )\n    print(\"Training set shape:\", X_train.shape)\n    print(\"Test set shape:\", X_test.shape)\n    \n    # -------------------------------------------------------\n    # 4. Load Pre-trained Base Models from Disk\n    # -------------------------------------------------------\n    print(\"Step 4: Loading pre-trained base models...\")\n    base_model_dir = \"/kaggle/input/basemodels\"\n    model_names = [\"LogisticRegression\", \"SVC\", \"XGBoost\", \"LightGBM\"]\n    \n    def load_base_models(model_dir, model_names):\n        models = []\n        for name in model_names:\n            model_path = os.path.join(model_dir, f\"{name}.pkl\")\n            with open(model_path, \"rb\") as f:\n                model = pickle.load(f)\n            models.append((name, model))\n            print(f\"Loaded {name} model from {model_path}\")\n        return models\n    \n    base_models = load_base_models(base_model_dir, model_names)\n    \n    # -------------------------------------------------------\n    # 5. Build a Voting Ensemble Model (Soft Voting)\n    # -------------------------------------------------------\n    print(\"Step 5: Building voting ensemble...\")\n    voting_clf = VotingClassifier(estimators=base_models, voting='soft')\n    \n    # -------------------------------------------------------\n    # 6. Evaluate Ensemble with Manual Cross-Validation\n    # -------------------------------------------------------\n    print(\"Step 6: Evaluating ensemble with cross-validation...\")\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = []\n    fold_idx = 1\n    for train_idx, val_idx in tqdm(skf.split(X_train, y_train), total=skf.get_n_splits(), desc=\"CV Folds\"):\n        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n        # Train ensemble on the fold training data\n        voting_clf.fit(X_tr, y_tr)\n        score = accuracy_score(y_val, voting_clf.predict(X_val))\n        cv_scores.append(score)\n        print(f\"Fold {fold_idx} accuracy: {score:.4f}\")\n        fold_idx += 1\n    print(\"\\nVoting Ensemble CV Accuracy: {:.4f}\".format(np.mean(cv_scores)))\n    \n    print(\"Step 7: Training voting ensemble on full training set...\")\n    voting_clf.fit(X_train, y_train)\n    \n    # -------------------------------------------------------\n    # 7. Evaluate Model Performance on the Test Set\n    # -------------------------------------------------------\n    print(\"Step 8: Evaluating ensemble on test set...\")\n    y_pred = voting_clf.predict(X_test)\n    y_prob = voting_clf.predict_proba(X_test)\n    \n    test_acc = accuracy_score(y_test, y_pred)\n    print(\"\\nTest Accuracy: {:.4f}\".format(test_acc))\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred, target_names=le.classes_))\n    \n    print(\"Confusion Matrix:\")\n    cm = confusion_matrix(y_test, y_pred)\n    print(cm)\n    \n    try:\n        auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')\n    except Exception as e:\n        auc = None\n    print(\"AUC-ROC:\", auc)\n    \n    test_ll = log_loss(y_test, y_prob)\n    print(\"Log Loss:\", test_ll)\n    \n    # -------------------------------------------------------\n    # 8. Check if Model is Online (Sample Prediction)\n    # -------------------------------------------------------\n    print(\"Step 9: Checking if model is online...\")\n    sample_input = X_test[0].reshape(1, -1)\n    sample_pred = voting_clf.predict(sample_input)\n    print(\"Sample prediction for first test sample:\", sample_pred)\n    print(\"Model is online and ready for predictions!\")\n    \n    # -------------------------------------------------------\n    # 9. Save the Final Ensemble Model\n    # -------------------------------------------------------\n    print(\"Step 10: Saving final ensemble model...\")\n    ensemble_model_path = \"/kaggle/working/voting_ensemble.pkl\"\n    with open(ensemble_model_path, \"wb\") as f:\n        pickle.dump(voting_clf, f)\n    print(\"Voting ensemble model saved to:\", ensemble_model_path)\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T11:31:18.012618Z","iopub.execute_input":"2025-02-21T11:31:18.012974Z"},"collapsed":true,"jupyter":{"source_hidden":true,"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Step 1: Merging fused selected features and labels...\n","output_type":"stream"},{"name":"stderr","text":"Merging fused feature files: 100%|██████████| 8/8 [00:01<00:00,  4.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Merged fused features shape: (7909, 4608)\nMerged labels shape: (7909,)\nStep 2: Encoding class labels...\nUnique classes: ['benign_adenosis' 'benign_fibroadenoma' 'benign_phyllodes_tumor'\n 'benign_tubular_adenoma' 'malignant_ductal_carcinoma'\n 'malignant_lobular_carcinoma' 'malignant_mucinous_carcinoma'\n 'malignant_papillary_carcinoma']\nStep 3: Splitting data into training and test sets...\nTraining set shape: (6327, 4608)\nTest set shape: (1582, 4608)\nStep 4: Loading pre-trained base models...\nLoaded LogisticRegression model from /kaggle/input/basemodels/LogisticRegression.pkl\nLoaded SVC model from /kaggle/input/basemodels/SVC.pkl\nLoaded ExtraTrees model from /kaggle/input/basemodels/ExtraTrees.pkl\nLoaded CalibratedRidge model from /kaggle/input/basemodels/CalibratedRidge.pkl\nLoaded XGBoost model from /kaggle/input/basemodels/XGBoost.pkl\nLoaded LightGBM model from /kaggle/input/basemodels/LightGBM.pkl\nStep 5: Building voting ensemble...\nStep 6: Evaluating ensemble with cross-validation...\n","output_type":"stream"},{"name":"stderr","text":"CV Folds:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.556908 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1174919\n[LightGBM] [Info] Number of data points in the train set: 5061, number of used features: 4608\n[LightGBM] [Info] Start training from score -2.880345\n[LightGBM] [Info] Start training from score -2.055429\n[LightGBM] [Info] Start training from score -2.862893\n[LightGBM] [Info] Start training from score -2.632166\n[LightGBM] [Info] Start training from score -0.829477\n[LightGBM] [Info] Start training from score -2.535358\n[LightGBM] [Info] Start training from score -2.298838\n[LightGBM] [Info] Start training from score -2.645997\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pickle\nimport warnings\nimport time\nfrom tqdm import tqdm\nfrom imblearn.over_sampling import SMOTE\n\n# Scikit-learn imports\nfrom sklearn.preprocessing import RobustScaler, PowerTransformer, LabelEncoder\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\n\nimport logging, sys\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\ndef merge_fused_features_labels(input_dir):\n    \"\"\"\n    Scans the given directory for files starting with 'fused_selected_features_'\n    and loads their corresponding 'fused_selected_labels_' files.\n    Returns:\n        all_features: NumPy array of shape (N, D)\n        all_labels: NumPy array of shape (N,)\n    \"\"\"\n    print(f\"Merging fused feature files from: {input_dir}\")\n    feat_files = glob.glob(os.path.join(input_dir, \"fused_selected_features_*.npy\"))\n    features_list = []\n    labels_list = []\n    for feat_file in tqdm(feat_files, desc=\"Merging fused feature files\"):\n        label_file = feat_file.replace(\"fused_selected_features_\", \"fused_selected_labels_\")\n        if os.path.exists(label_file):\n            feats = np.load(feat_file)\n            labs = np.load(label_file)\n            features_list.append(feats)\n            labels_list.append(labs)\n            print(f\"Loaded {os.path.basename(feat_file)} with shape {feats.shape}\")\n        else:\n            logger.warning(f\"Label file for {feat_file} not found.\")\n    if features_list:\n        all_features = np.concatenate(features_list, axis=0)\n        all_labels = np.concatenate(labels_list, axis=0)\n        print(\"Merging completed.\")\n        return all_features, all_labels\n    else:\n        return None, None\n\ndef preprocess_features(X, feature_threshold=0.95):\n    \"\"\"\n    Preprocess features by removing quasi-constant features, robust scaling,\n    and applying Yeo-Johnson transformation.\n    \"\"\"\n    print(\"Preprocessing: Removing quasi-constant features...\")\n    selector = VarianceThreshold(threshold=0.01)\n    X = selector.fit_transform(X)\n    \n    print(\"Preprocessing: Applying RobustScaler...\")\n    scaler = RobustScaler()\n    X = scaler.fit_transform(X)\n    \n    print(\"Preprocessing: Applying PowerTransformer (Yeo-Johnson)...\")\n    power = PowerTransformer(method='yeo-johnson')\n    X = power.fit_transform(X)\n    \n    return X\n\n# Module A: Execute Data Loading and Preprocessing\nprint(\"Module A: Data Loading and Preprocessing started.\")\ninput_dir = \"/kaggle/input/selected-feats/fused_selected_features/\"\nall_features, all_labels = merge_fused_features_labels(input_dir)\n\nif all_features is None or all_labels is None:\n    logger.error(\"No features or labels found. Check the directory and file naming.\")\n    sys.exit(1)\n\nprint(f\"Merged features shape: {all_features.shape}\")\nprint(f\"Merged labels shape: {all_labels.shape}\")\n\nprint(\"Preprocessing features...\")\nall_features = preprocess_features(all_features)\nprint(f\"Preprocessed feature shape: {all_features.shape}\")\n\nprint(\"Encoding class labels...\")\nle = LabelEncoder()\ny_encoded = le.fit_transform(all_labels)\nprint(f\"Unique classes: {le.classes_}\")\n\nprint(\"Splitting data into training and test sets...\")\nX_train, X_test, y_train, y_test = train_test_split(\n    all_features, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n)\nprint(f\"Training set shape: {X_train.shape}\")\nprint(f\"Test set shape: {X_test.shape}\")\n\nprint(\"Applying SMOTE for class balancing on training data...\")\nsmote = SMOTE(random_state=42)\nX_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\nprint(f\"Balanced training set shape: {X_train_bal.shape}\")\n\n# Save preprocessed data if desired\nwith open(\"/kaggle/working/preprocessed_data.pkl\", \"wb\") as f:\n    pickle.dump((X_train_bal, y_train_bal, X_test, y_test, le), f)\n\nprint(\"Module A completed successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T08:08:03.737765Z","iopub.execute_input":"2025-02-22T08:08:03.738182Z","iopub.status.idle":"2025-02-22T08:08:48.264032Z","shell.execute_reply.started":"2025-02-22T08:08:03.738145Z","shell.execute_reply":"2025-02-22T08:08:48.263350Z"}},"outputs":[{"name":"stderr","text":"Merging fused feature files: 100%|██████████| 8/8 [00:00<00:00, 78.62it/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(\"merged_fused_selected_features.npy\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T02:28:32.543428Z","iopub.execute_input":"2025-02-23T02:28:32.543867Z","iopub.status.idle":"2025-02-23T02:28:32.552250Z","shell.execute_reply.started":"2025-02-23T02:28:32.543830Z","shell.execute_reply":"2025-02-23T02:28:32.550885Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/merged_fused_selected_features.npy","text/html":"<a href='merged_fused_selected_features.npy' target='_blank'>merged_fused_selected_features.npy</a><br>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pickle\nimport warnings\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport logging, sys\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\ndef merge_fused_features_labels(input_dir):\n    \"\"\"\n    Scans the given directory for files starting with \n    'fused_selected_features_' and loads their corresponding\n    'fused_selected_labels_' files.\n    \n    Returns:\n        all_features: NumPy array of shape (N, D)\n        all_labels: NumPy array of shape (N,)\n    \"\"\"\n    print(f\"Merging fused feature files from: {input_dir}\")\n    feat_files = glob.glob(os.path.join(input_dir, \"fused_selected_features_*.npy\"))\n    features_list = []\n    labels_list = []\n    \n    for feat_file in tqdm(feat_files, desc=\"Merging fused feature files\"):\n        label_file = feat_file.replace(\"fused_selected_features_\", \"fused_selected_labels_\")\n        if os.path.exists(label_file):\n            feats = np.load(feat_file)\n            labs = np.load(label_file)\n            features_list.append(feats)\n            labels_list.append(labs)\n            print(f\"Loaded {os.path.basename(feat_file)} with shape {feats.shape}\")\n        else:\n            logger.warning(f\"Label file for {feat_file} not found.\")\n    \n    if features_list:\n        all_features = np.concatenate(features_list, axis=0)\n        all_labels = np.concatenate(labels_list, axis=0)\n        print(\"Merging completed.\")\n        return all_features, all_labels\n    else:\n        return None, None\n\ndef main():\n    # -------------------------------------------------------\n    # 1. Merge All Fused Selected Feature Files and Labels\n    # -------------------------------------------------------\n    input_dir = \"/kaggle/input/selected-feats/fused_selected_features/\"\n    all_features, all_labels = merge_fused_features_labels(input_dir)\n    \n    if all_features is None or all_labels is None:\n        logger.error(\"No features or labels found. Please check the directory.\")\n        return\n    \n    # Save merged features and labels for later reference (optional)\n    merged_feat_path = \"/kaggle/working/merged_fused_selected_features.npy\"\n    merged_labels_path = \"/kaggle/working/merged_labels.npy\"\n    np.save(merged_feat_path, all_features)\n    np.save(merged_labels_path, all_labels)\n    print(f\"Merged features saved to: {merged_feat_path}\")\n    print(f\"Merged labels saved to: {merged_labels_path}\")\n    \n    print(f\"Merged fused features shape: {all_features.shape}\")\n    print(f\"Merged labels shape: {all_labels.shape}\")\n    \n    # -------------------------------------------------------\n    # 2. Encode Class Labels into Numeric Values\n    # -------------------------------------------------------\n    print(\"Encoding class labels...\")\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(all_labels)\n    print(f\"Unique classes: {le.classes_}\")\n    \n    # -------------------------------------------------------\n    # 3. Split Data into Training and Testing Sets (Stratified)\n    # -------------------------------------------------------\n    print(\"Splitting data into training and test sets...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        all_features, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n    )\n    print(f\"Training set shape: {X_train.shape}\")\n    print(f\"Test set shape: {X_test.shape}\")\n    \n    # -------------------------------------------------------\n    # 4. Load Pre-trained Base Models from Disk\n    # -------------------------------------------------------\n    base_model_dir = \"/kaggle/input/basemodels\"\n    model_names = [\"LogisticRegression\", \"SVC\", \"ExtraTrees\", \"CalibratedRidge\", \"XGBoost\"]\n    def load_base_models(model_dir, model_names):\n        models = []\n        for name in model_names:\n            model_path = os.path.join(model_dir, f\"{name}.pkl\")\n            with open(model_path, \"rb\") as f:\n                model = pickle.load(f)\n            models.append((name, model))\n            print(f\"Loaded model {name} from {model_path}\")\n        return models\n    base_models = load_base_models(base_model_dir, model_names)\n    \n    # -------------------------------------------------------\n    # 5. Build a Voting Ensemble Model (Soft Voting)\n    # -------------------------------------------------------\n    print(\"Building voting ensemble...\")\n    voting_clf = VotingClassifier(estimators=base_models, voting='soft', n_jobs=-1)\n    \n    # Evaluate using 5-fold stratified cross-validation\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    cv_scores = cross_val_score(voting_clf, X_train, y_train, cv=skf, scoring='accuracy', n_jobs=-1)\n    print(f\"\\nVoting Ensemble CV Accuracy: {cv_scores.mean():.4f}\")\n    \n    print(\"Training voting ensemble on full training set...\")\n    voting_clf.fit(X_train, y_train)\n    \n    # -------------------------------------------------------\n    # 6. Evaluate Model Performance on the Test Set\n    # -------------------------------------------------------\n    print(\"Evaluating ensemble on test set...\")\n    y_pred = voting_clf.predict(X_test)\n    y_prob = voting_clf.predict_proba(X_test)\n    \n    acc = accuracy_score(y_test, y_pred)\n    print(f\"\\nTest Accuracy: {acc:.4f}\")\n    \n    print(\"\\nClassification Report:\")\n    print(\"\\n\" + classification_report(y_test, y_pred, target_names=le.classes_))\n    \n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\")\n    print(\"\\n\" + str(cm))\n    \n    try:\n        auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')\n    except Exception as e:\n        auc = None\n    print(f\"AUC-ROC: {auc}\")\n    \n    ll = log_loss(y_test, y_prob)\n    print(f\"Log Loss: {ll}\")\n    \n    # -------------------------------------------------------\n    # 7. Check if Model is Online (Sample Prediction)\n    # -------------------------------------------------------\n    sample_input = X_test[0].reshape(1, -1)\n    sample_pred = voting_clf.predict(sample_input)\n    print(f\"\\nSample prediction for first test sample: {sample_pred}\")\n    print(\"Model is online and ready for predictions!\")\n    \n    # -------------------------------------------------------\n    # 8. Save the Final Ensemble Model\n    # -------------------------------------------------------\n    ensemble_model_path = \"/kaggle/working/voting_ensemble.pkl\"\n    with open(ensemble_model_path, \"wb\") as f:\n        pickle.dump(voting_clf, f)\n    print(f\"Voting ensemble model saved to: {ensemble_model_path}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T15:01:38.876655Z","iopub.execute_input":"2025-02-22T15:01:38.876988Z","execution_failed":"2025-02-22T16:12:30.464Z"}},"outputs":[{"name":"stdout","text":"Merging fused feature files from: /kaggle/input/selected-feats/fused_selected_features/\n","output_type":"stream"},{"name":"stderr","text":"Merging fused feature files: 100%|██████████| 8/8 [00:00<00:00, 89.66it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded fused_selected_features_benign_fibroadenoma.npy with shape (1014, 4608)\nLoaded fused_selected_features_benign_tubular_adenoma.npy with shape (569, 4608)\nLoaded fused_selected_features_malignant_lobular_carcinoma.npy with shape (626, 4608)\nLoaded fused_selected_features_malignant_ductal_carcinoma.npy with shape (3451, 4608)\nLoaded fused_selected_features_benign_adenosis.npy with shape (444, 4608)\nLoaded fused_selected_features_benign_phyllodes_tumor.npy with shape (453, 4608)\nLoaded fused_selected_features_malignant_mucinous_carcinoma.npy with shape (792, 4608)\nLoaded fused_selected_features_malignant_papillary_carcinoma.npy with shape (560, 4608)\nMerging completed.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Merged features saved to: /kaggle/working/merged_fused_selected_features.npy\nMerged labels saved to: /kaggle/working/merged_labels.npy\nMerged fused features shape: (7909, 4608)\nMerged labels shape: (7909,)\nEncoding class labels...\nUnique classes: ['benign_adenosis' 'benign_fibroadenoma' 'benign_phyllodes_tumor'\n 'benign_tubular_adenoma' 'malignant_ductal_carcinoma'\n 'malignant_lobular_carcinoma' 'malignant_mucinous_carcinoma'\n 'malignant_papillary_carcinoma']\nSplitting data into training and test sets...\nTraining set shape: (6327, 4608)\nTest set shape: (1582, 4608)\nLoaded model LogisticRegression from /kaggle/input/basemodels/LogisticRegression.pkl\nLoaded model SVC from /kaggle/input/basemodels/SVC.pkl\nLoaded model ExtraTrees from /kaggle/input/basemodels/ExtraTrees.pkl\nLoaded model CalibratedRidge from /kaggle/input/basemodels/CalibratedRidge.pkl\nLoaded model XGBoost from /kaggle/input/basemodels/XGBoost.pkl\nBuilding voting ensemble...\n\nVoting Ensemble CV Accuracy: 0.8132\nTraining voting ensemble on full training set...\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport numpy as np\nimport pickle\nimport warnings\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nimport logging, sys\n\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[logging.StreamHandler(sys.stdout)]\n)\nlogger = logging.getLogger(__name__)\n\ndef merge_fused_features_labels(input_dir):\n    \"\"\"\n    Scans the given directory for files starting with \n    'fused_selected_features_' and loads their corresponding\n    'fused_selected_labels_' files.\n    \n    Returns:\n        all_features: NumPy array of shape (N, D)\n        all_labels: NumPy array of shape (N,)\n    \"\"\"\n    print(f\"Merging fused feature files from: {input_dir}\")\n    feat_files = glob.glob(os.path.join(input_dir, \"fused_selected_features_*.npy\"))\n    features_list = []\n    labels_list = []\n    \n    for feat_file in tqdm(feat_files, desc=\"Merging fused feature files\"):\n        label_file = feat_file.replace(\"fused_selected_features_\", \"fused_selected_labels_\")\n        if os.path.exists(label_file):\n            feats = np.load(feat_file)\n            labs = np.load(label_file)\n            features_list.append(feats)\n            labels_list.append(labs)\n            print(f\"Loaded {os.path.basename(feat_file)} with shape {feats.shape}\")\n        else:\n            logger.warning(f\"Label file for {feat_file} not found.\")\n    \n    if features_list:\n        all_features = np.concatenate(features_list, axis=0)\n        all_labels = np.concatenate(labels_list, axis=0)\n        print(\"Merging completed.\")\n        return all_features, all_labels\n    else:\n        return None, None\n\ndef main():\n    # -------------------------------------------------------\n    # 1. Merge All Fused Selected Feature Files and Labels\n    # -------------------------------------------------------\n    input_dir = \"/kaggle/input/selected-feats/fused_selected_features/\"\n    all_features, all_labels = merge_fused_features_labels(input_dir)\n    \n    if all_features is None or all_labels is None:\n        logger.error(\"No features or labels found. Please check the directory.\")\n        return\n    \n    # Save merged features and labels for later reference (optional)\n    merged_feat_path = \"/kaggle/working/merged_fused_selected_features.npy\"\n    merged_labels_path = \"/kaggle/working/merged_labels.npy\"\n    np.save(merged_feat_path, all_features)\n    np.save(merged_labels_path, all_labels)\n    print(f\"Merged features saved to: {merged_feat_path}\")\n    print(f\"Merged labels saved to: {merged_labels_path}\")\n    \n    print(f\"Merged fused features shape: {all_features.shape}\")\n    print(f\"Merged labels shape: {all_labels.shape}\")\n    \n    # -------------------------------------------------------\n    # 2. Encode Class Labels into Numeric Values\n    # -------------------------------------------------------\n    print(\"Encoding class labels...\")\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(all_labels)\n    print(f\"Unique classes: {le.classes_}\")\n    \n    # -------------------------------------------------------\n    # 3. Split Data into Training and Testing Sets (Stratified)\n    # -------------------------------------------------------\n    print(\"Splitting data into training and test sets...\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        all_features, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded\n    )\n    print(f\"Training set shape: {X_train.shape}\")\n    print(f\"Test set shape: {X_test.shape}\")\n    \n    # -------------------------------------------------------\n    # 4. Load Pre-trained Base Models from Disk\n    # -------------------------------------------------------\n    base_model_dir = \"/kaggle/input/basemodels\"\n    model_names = [\"LogisticRegression\", \"SVC\", \"ExtraTrees\", \"CalibratedRidge\", \"XGBoost\"]\n    def load_base_models(model_dir, model_names):\n        models = []\n        for name in model_names:\n            model_path = os.path.join(model_dir, f\"{name}.pkl\")\n            with open(model_path, \"rb\") as f:\n                model = pickle.load(f)\n            models.append((name, model))\n            print(f\"Loaded model {name} from {model_path}\")\n        return models\n    base_models = load_base_models(base_model_dir, model_names)\n    \n    # -------------------------------------------------------\n    # 5. Build a Voting Ensemble Model (Soft Voting)\n    # -------------------------------------------------------\n    print(\"Building voting ensemble...\")\n    voting_clf = VotingClassifier(estimators=base_models, voting='soft', n_jobs=-1)\n\n    print(\"Training voting ensemble on full training set...\")\n    voting_clf.fit(X_train, y_train)\n    \n    # -------------------------------------------------------\n    # 6. Evaluate Model Performance on the Test Set\n    # -------------------------------------------------------\n    print(\"Evaluating ensemble on test set...\")\n    y_pred = voting_clf.predict(X_test)\n    y_prob = voting_clf.predict_proba(X_test)\n    \n    acc = accuracy_score(y_test, y_pred)\n    print(f\"\\nTest Accuracy: {acc:.4f}\")\n    \n    print(\"\\nClassification Report:\")\n    print(\"\\n\" + classification_report(y_test, y_pred, target_names=le.classes_))\n    \n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\")\n    print(\"\\n\" + str(cm))\n    \n    try:\n        auc = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')\n    except Exception as e:\n        auc = None\n    print(f\"AUC-ROC: {auc}\")\n    \n    ll = log_loss(y_test, y_prob)\n    print(f\"Log Loss: {ll}\")\n    \n    # -------------------------------------------------------\n    # 7. Check if Model is Online (Sample Prediction)\n    # -------------------------------------------------------\n    sample_input = X_test[0].reshape(1, -1)\n    sample_pred = voting_clf.predict(sample_input)\n    print(f\"\\nSample prediction for first test sample: {sample_pred}\")\n    print(\"Model is online and ready for predictions!\")\n    \n    # -------------------------------------------------------\n    # 8. Save the Final Ensemble Model\n    # -------------------------------------------------------\n    ensemble_model_path = \"/kaggle/working/voting_ensemble.pkl\"\n    with open(ensemble_model_path, \"wb\") as f:\n        pickle.dump(voting_clf, f)\n    print(f\"Voting ensemble model saved to: {ensemble_model_path}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T16:14:59.206698Z","iopub.execute_input":"2025-02-22T16:14:59.209477Z","iopub.status.idle":"2025-02-22T16:38:19.560192Z","shell.execute_reply.started":"2025-02-22T16:14:59.209385Z","shell.execute_reply":"2025-02-22T16:38:19.558951Z"}},"outputs":[{"name":"stdout","text":"Merging fused feature files from: /kaggle/input/selected-feats/fused_selected_features/\n","output_type":"stream"},{"name":"stderr","text":"Merging fused feature files:  25%|██▌       | 2/8 [00:00<00:01,  5.26it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded fused_selected_features_benign_fibroadenoma.npy with shape (1014, 4608)\nLoaded fused_selected_features_benign_tubular_adenoma.npy with shape (569, 4608)\n","output_type":"stream"},{"name":"stderr","text":"Merging fused feature files:  38%|███▊      | 3/8 [00:00<00:00,  6.23it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded fused_selected_features_malignant_lobular_carcinoma.npy with shape (626, 4608)\n","output_type":"stream"},{"name":"stderr","text":"Merging fused feature files:  75%|███████▌  | 6/8 [00:01<00:00,  4.96it/s]","output_type":"stream"},{"name":"stdout","text":"Loaded fused_selected_features_malignant_ductal_carcinoma.npy with shape (3451, 4608)\nLoaded fused_selected_features_benign_adenosis.npy with shape (444, 4608)\nLoaded fused_selected_features_benign_phyllodes_tumor.npy with shape (453, 4608)\n","output_type":"stream"},{"name":"stderr","text":"Merging fused feature files: 100%|██████████| 8/8 [00:01<00:00,  5.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded fused_selected_features_malignant_mucinous_carcinoma.npy with shape (792, 4608)\nLoaded fused_selected_features_malignant_papillary_carcinoma.npy with shape (560, 4608)\nMerging completed.\nMerged features saved to: /kaggle/working/merged_fused_selected_features.npy\nMerged labels saved to: /kaggle/working/merged_labels.npy\nMerged fused features shape: (7909, 4608)\nMerged labels shape: (7909,)\nEncoding class labels...\nUnique classes: ['benign_adenosis' 'benign_fibroadenoma' 'benign_phyllodes_tumor'\n 'benign_tubular_adenoma' 'malignant_ductal_carcinoma'\n 'malignant_lobular_carcinoma' 'malignant_mucinous_carcinoma'\n 'malignant_papillary_carcinoma']\nSplitting data into training and test sets...\nTraining set shape: (6327, 4608)\nTest set shape: (1582, 4608)\nLoaded model LogisticRegression from /kaggle/input/basemodels/LogisticRegression.pkl\nLoaded model SVC from /kaggle/input/basemodels/SVC.pkl\nLoaded model ExtraTrees from /kaggle/input/basemodels/ExtraTrees.pkl\nLoaded model CalibratedRidge from /kaggle/input/basemodels/CalibratedRidge.pkl\nLoaded model XGBoost from /kaggle/input/basemodels/XGBoost.pkl\nBuilding voting ensemble...\nTraining voting ensemble on full training set...\nEvaluating ensemble on test set...\n\nTest Accuracy: 0.8262\n\nClassification Report:\n\n                               precision    recall  f1-score   support\n\n              benign_adenosis       0.89      0.85      0.87        89\n          benign_fibroadenoma       0.82      0.84      0.83       203\n       benign_phyllodes_tumor       0.89      0.75      0.81        91\n       benign_tubular_adenoma       0.94      0.89      0.91       114\n   malignant_ductal_carcinoma       0.81      0.94      0.87       690\n  malignant_lobular_carcinoma       0.62      0.42      0.50       125\n malignant_mucinous_carcinoma       0.85      0.73      0.78       158\nmalignant_papillary_carcinoma       0.91      0.66      0.77       112\n\n                     accuracy                           0.83      1582\n                    macro avg       0.84      0.76      0.79      1582\n                 weighted avg       0.83      0.83      0.82      1582\n\nConfusion Matrix:\n\n[[ 76   3   0   2   1   0   7   0]\n [  2 170   7   1  20   0   3   0]\n [  0  16  68   0   7   0   0   0]\n [  0  10   0 101   0   0   2   1]\n [  3   1   1   1 650  26   4   4]\n [  0   0   0   0  66  53   4   2]\n [  3   5   0   2  27   6 115   0]\n [  1   2   0   0  34   0   1  74]]\nAUC-ROC: 0.976669432621839\nLog Loss: 0.5767611522926458\n\nSample prediction for first test sample: [4]\nModel is online and ready for predictions!\nVoting ensemble model saved to: /kaggle/working/voting_ensemble.pkl\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip freeze > requirements_kaggle.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T07:45:36.596982Z","iopub.execute_input":"2025-03-09T07:45:36.597377Z","iopub.status.idle":"2025-03-09T07:45:39.196161Z","shell.execute_reply.started":"2025-03-09T07:45:36.597347Z","shell.execute_reply":"2025-03-09T07:45:39.194119Z"}},"outputs":[],"execution_count":1}]}